
<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>lightning.impl.sag &mdash; lightning dev documentation</title>
    
    <link rel="stylesheet" href="../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/lightning.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/bootstrap-3.3.4/css/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/bootstrap-3.3.4/css/bootstrap-theme.min.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/bootstrap-sphinx.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     'dev',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../../../_static/bootstrap-3.3.4/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../../../_static/bootstrap-sphinx.js"></script>
    <link rel="top" title="lightning dev documentation" href="../../../index.html" />
    <link rel="up" title="Module code" href="../../index.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body role="document">

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../index.html">
          lightning</a>
        <span class="navbar-text navbar-version pull-left"><b>dev</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../../../intro.html">Introduction</a></li>
                <li><a href="../../../references.html">References</a></li>
                <li><a href="../../../auto_examples/index.html">Examples</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="../../../auto_examples/index.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../auto_examples/plot_1d_total_variation.html">Signal recovery by 1D total variation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../auto_examples/plot_sgd_loss_functions.html">SGD: Convex Loss Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../auto_examples/plot_robust_regression.html">Robust regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../auto_examples/trace.html">Trace norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../auto_examples/plot_sample_weight.html">SAGA: Weighted samples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../auto_examples/document_classification_news20.html">Classification of text documents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../auto_examples/plot_svrg.html">Sensitivity to hyper-parameters in SVRG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../auto_examples/plot_sparse_non_linear.html">Sparse non-linear classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../auto_examples/plot_l2_solvers.html">L2 solver comparison</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../references.html">Classification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.classification.AdaGradClassifier.html">lightning.classification.AdaGradClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.classification.CDClassifier.html">lightning.classification.CDClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.classification.FistaClassifier.html">lightning.classification.FistaClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.classification.KernelSVC.html">lightning.classification.KernelSVC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.classification.LinearSVC.html">lightning.classification.LinearSVC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.classification.SDCAClassifier.html">lightning.classification.SDCAClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.classification.SAGClassifier.html">lightning.classification.SAGClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.classification.SAGAClassifier.html">lightning.classification.SAGAClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.classification.SGDClassifier.html">lightning.classification.SGDClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.classification.SVRGClassifier.html">lightning.classification.SVRGClassifier</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../references.html#regression">Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.regression.AdaGradRegressor.html">lightning.regression.AdaGradRegressor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.regression.CDRegressor.html">lightning.regression.CDRegressor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.regression.FistaRegressor.html">lightning.regression.FistaRegressor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.regression.LinearSVR.html">lightning.regression.LinearSVR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.regression.SAGRegressor.html">lightning.regression.SAGRegressor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.regression.SAGARegressor.html">lightning.regression.SAGARegressor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.regression.SDCARegressor.html">lightning.regression.SDCARegressor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.regression.SGDRegressor.html">lightning.regression.SGDRegressor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.regression.SVRGRegressor.html">lightning.regression.SVRGRegressor</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../references.html#ranking">Ranking</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.ranking.PRank.html">lightning.ranking.PRank</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../generated/lightning.ranking.KernelPRank.html">lightning.ranking.KernelPRank</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../intro.html">Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../intro.html#primal-coordinate-descent">Primal coordinate descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../intro.html#dual-coordinate-ascent">Dual coordinate ascent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../intro.html#fista">FISTA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../intro.html#stochastic-gradient-method-sgd">Stochastic gradient method (SGD)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../intro.html#adagrad">AdaGrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../intro.html#stochastic-averaged-gradient-sag-and-saga">Stochastic averaged gradient (SAG and SAGA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../intro.html#stochastic-variance-reduced-gradient-svrg">Stochastic variance-reduced gradient (SVRG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../intro.html#prank">PRank</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../auto_examples/index.html">Examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../auto_examples/plot_1d_total_variation.html">Signal recovery by 1D total variation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../auto_examples/plot_sgd_loss_functions.html">SGD: Convex Loss Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../auto_examples/plot_robust_regression.html">Robust regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../auto_examples/trace.html">Trace norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../auto_examples/plot_sample_weight.html">SAGA: Weighted samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../auto_examples/document_classification_news20.html">Classification of text documents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../auto_examples/plot_svrg.html">Sensitivity to hyper-parameters in SVRG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../auto_examples/plot_sparse_non_linear.html">Sparse non-linear classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../auto_examples/plot_l2_solvers.html">L2 solver comparison</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../references.html">Classification</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.classification.AdaGradClassifier.html">lightning.classification.AdaGradClassifier</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.classification.CDClassifier.html">lightning.classification.CDClassifier</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.classification.FistaClassifier.html">lightning.classification.FistaClassifier</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.classification.KernelSVC.html">lightning.classification.KernelSVC</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.classification.LinearSVC.html">lightning.classification.LinearSVC</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.classification.SDCAClassifier.html">lightning.classification.SDCAClassifier</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.classification.SAGClassifier.html">lightning.classification.SAGClassifier</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.classification.SAGAClassifier.html">lightning.classification.SAGAClassifier</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.classification.SGDClassifier.html">lightning.classification.SGDClassifier</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.classification.SVRGClassifier.html">lightning.classification.SVRGClassifier</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../references.html#regression">Regression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.regression.AdaGradRegressor.html">lightning.regression.AdaGradRegressor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.regression.CDRegressor.html">lightning.regression.CDRegressor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.regression.FistaRegressor.html">lightning.regression.FistaRegressor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.regression.LinearSVR.html">lightning.regression.LinearSVR</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.regression.SAGRegressor.html">lightning.regression.SAGRegressor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.regression.SAGARegressor.html">lightning.regression.SAGARegressor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.regression.SDCARegressor.html">lightning.regression.SDCARegressor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.regression.SGDRegressor.html">lightning.regression.SGDRegressor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.regression.SVRGRegressor.html">lightning.regression.SVRGRegressor</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../references.html#ranking">Ranking</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.ranking.PRank.html">lightning.ranking.PRank</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../generated/lightning.ranking.KernelPRank.html">lightning.ranking.KernelPRank</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container content-container">
  
  <h1>Source code for lightning.impl.sag</h1><div class="highlight"><pre>
<span></span><span class="c1"># Author: Mathieu Blondel</span>
<span class="c1">#         Arnaud Rachez</span>
<span class="c1">#         Fabian Pedregosa</span>
<span class="c1"># License: BSD</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">sklearn.utils.extmath</span> <span class="kn">import</span> <span class="n">row_norms</span>
<span class="kn">from</span> <span class="nn">sklearn.externals.six.moves</span> <span class="kn">import</span> <span class="nb">xrange</span>

<span class="kn">from</span> <span class="nn">.base</span> <span class="kn">import</span> <span class="n">BaseClassifier</span><span class="p">,</span> <span class="n">BaseRegressor</span>
<span class="kn">from</span> <span class="nn">.dataset_fast</span> <span class="kn">import</span> <span class="n">get_dataset</span>
<span class="kn">from</span> <span class="nn">.sag_fast</span> <span class="kn">import</span> <span class="n">_sag_fit</span>

<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="kn">import</span> <span class="n">ModifiedHuber</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="kn">import</span> <span class="n">SmoothHinge</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="kn">import</span> <span class="n">SquaredHinge</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="kn">import</span> <span class="n">Log</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="kn">import</span> <span class="n">SquaredLoss</span>
<span class="kn">from</span> <span class="nn">.sag_fast</span> <span class="kn">import</span> <span class="n">L1Penalty</span>


<span class="k">def</span> <span class="nf">get_auto_step_size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute automatic step size for SAG solver</span>
<span class="sd">    Stepsize computed using the following objective:</span>
<span class="sd">        minimize_w  1 / n_samples * \sum_i loss(w^T x_i, y_i)</span>
<span class="sd">                    + alpha * 0.5 * ||w||^2_2</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : ndarray</span>
<span class="sd">        Array of samples x_i.</span>
<span class="sd">    alpha : float</span>
<span class="sd">        Constant that multiplies the l2 penalty term.</span>
<span class="sd">    loss : string, in {&quot;log&quot;, &quot;squared&quot;}</span>
<span class="sd">        The loss function used in SAG solver.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    step_size : float</span>
<span class="sd">        Step size used in SAG/SAGA solver.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">weighted_norms</span> <span class="o">=</span> <span class="n">row_norms</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">weighted_norms</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">row_norms</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">weighted_norms</span><span class="p">)</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;log&#39;</span><span class="p">:</span>
        <span class="c1"># inverse Lipschitz constant for log loss</span>
        <span class="n">lipschitz_constant</span> <span class="o">=</span> <span class="mf">0.25</span> <span class="o">*</span> <span class="n">L</span> <span class="o">+</span> <span class="n">alpha</span>
    <span class="k">elif</span> <span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;squared&#39;</span><span class="p">:</span>
        <span class="n">lipschitz_constant</span> <span class="o">=</span> <span class="n">L</span> <span class="o">+</span> <span class="n">alpha</span>
    <span class="k">elif</span> <span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;modified_huber&#39;</span><span class="p">:</span>
        <span class="n">lipschitz_constant</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">L</span> <span class="o">+</span> <span class="n">alpha</span>
    <span class="k">elif</span> <span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;smooth_hinge&#39;</span><span class="p">:</span>
        <span class="n">lipschitz_constant</span> <span class="o">=</span> <span class="n">L</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">alpha</span>
    <span class="k">elif</span> <span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;squared_hinge&#39;</span><span class="p">:</span>
        <span class="n">lipschitz_constant</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">L</span> <span class="o">+</span> <span class="n">alpha</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`auto` stepsize is only available for `squared` or &quot;</span>
                         <span class="s2">&quot;`log` losses (got `</span><span class="si">%s</span><span class="s2">` loss). Please specify a &quot;</span>
                         <span class="s2">&quot;stepsize.&quot;</span> <span class="o">%</span> <span class="n">loss</span><span class="p">)</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">lipschitz_constant</span>


<span class="k">class</span> <span class="nc">_BaseSAG</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">_get_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;modified_huber&quot;</span><span class="p">:</span> <span class="n">ModifiedHuber</span><span class="p">(),</span>
            <span class="s2">&quot;smooth_hinge&quot;</span><span class="p">:</span> <span class="n">SmoothHinge</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">),</span>
            <span class="s2">&quot;squared_hinge&quot;</span><span class="p">:</span> <span class="n">SquaredHinge</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span>
            <span class="s2">&quot;log&quot;</span><span class="p">:</span> <span class="n">Log</span><span class="p">(),</span>
            <span class="s2">&quot;squared&quot;</span><span class="p">:</span> <span class="n">SquaredLoss</span><span class="p">(),</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">losses</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_get_penalty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="c1"># l2 penalty is governed by the alpha keyword in `_sag_fit`.</span>
            <span class="c1"># beta governs the strength of the penalties below.</span>
            <span class="n">penalties</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;l1&quot;</span><span class="p">:</span> <span class="n">L1Penalty</span><span class="p">(),</span>
            <span class="p">}</span>
            <span class="k">return</span> <span class="n">penalties</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span>

    <span class="k">def</span> <span class="nf">_finalize_coef</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_scale_</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_scale_</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_random_state</span><span class="p">()</span>
        <span class="n">adaptive_step_size</span> <span class="o">=</span> <span class="bp">False</span>

        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="s1">&#39;line-search&#39;</span><span class="p">):</span>
            <span class="n">step_size</span> <span class="o">=</span> <span class="n">get_auto_step_size</span><span class="p">(</span>
                    <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Auto stepsize: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">==</span> <span class="s1">&#39;line-search&#39;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">step_size</span>
                <span class="n">adaptive_step_size</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">step_size</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_loss</span><span class="p">()</span>
        <span class="n">penalty</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_penalty</span><span class="p">()</span>
        <span class="n">n_vectors</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">n_inner</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_inner</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span>
        <span class="n">ds</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;c&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_vectors</span><span class="p">,</span> <span class="n">n_features</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_scale_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_vectors</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_vectors</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">n_vectors</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>

            <span class="n">_sag_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ds</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_scale_</span><span class="p">[</span><span class="n">i</span><span class="p">:],</span> <span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                     <span class="n">sample_weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">penalty</span><span class="p">,</span>
                     <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">n_inner</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                     <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="p">,</span> <span class="n">rng</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_saga</span><span class="p">,</span> <span class="n">adaptive_step_size</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>


<div class="viewcode-block" id="SAGClassifier"><a class="viewcode-back" href="../../../generated/lightning.classification.SAGClassifier.html#lightning.classification.SAGClassifier">[docs]</a><span class="k">class</span> <span class="nc">SAGClassifier</span><span class="p">(</span><span class="n">BaseClassifier</span><span class="p">,</span> <span class="n">_BaseSAG</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimator for learning linear classifiers by SAG.</span>

<span class="sd">    Solves the following objective:</span>

<span class="sd">        minimize_w  1 / n_samples * \sum_i loss(w^T x_i, y_i)</span>
<span class="sd">                    + alpha * 0.5 * ||w||^2_2</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    eta : float or {&#39;auto&#39;, &#39;line-search&#39;}, defaults to &#39;auto&#39;</span>
<span class="sd">        step size for the gradient updates. If set to &#39;auto&#39;,</span>
<span class="sd">        this will calculate a step size based on the input data.</span>
<span class="sd">        If set to &#39;line-search&#39;, it will perform a line-search</span>
<span class="sd">        to find the step size based for the current iteration.</span>
<span class="sd">    alpha : float</span>
<span class="sd">        amount of squared L2 regularization</span>
<span class="sd">    beta : float</span>
<span class="sd">        amount of regularization for the penalty term</span>
<span class="sd">    loss : string</span>
<span class="sd">        loss to use in the objective function. Can be one of</span>
<span class="sd">        &quot;smooth_hinge&quot;, &quot;squared_hinge&quot; or &quot;log&quot; (for logistic loss).</span>
<span class="sd">    gamma : float</span>
<span class="sd">        gamma parameter in the &quot;smooth_hinge&quot; loss (not used for other</span>
<span class="sd">        loss functions)</span>
<span class="sd">    max_iter : int</span>
<span class="sd">        maximum number of outer iterations (also known as epochs).</span>
<span class="sd">    tol : float</span>
<span class="sd">        stopping criterion tolerance.</span>
<span class="sd">    verbose : int</span>
<span class="sd">        verbosity level. Set positive to print progress information.</span>
<span class="sd">    callback : callable or None</span>
<span class="sd">        if given, callback(self) will be called on each outer iteration</span>
<span class="sd">        (epoch).</span>
<span class="sd">    random_state: int or RandomState</span>
<span class="sd">        Pseudo-random number generator state used for random sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="SAGClassifier.__init__"><a class="viewcode-back" href="../../../generated/lightning.classification.SAGClassifier.html#lightning.classification.SAGClassifier.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;smooth_hinge&quot;</span><span class="p">,</span>
                 <span class="n">penalty</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_inner</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">eta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">=</span> <span class="n">penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_inner</span> <span class="o">=</span> <span class="n">n_inner</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback</span> <span class="o">=</span> <span class="n">callback</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_saga</span> <span class="o">=</span> <span class="bp">False</span></div>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_saga</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Penalties in SAGClassifier. Please use &#39;</span>
                             <span class="s1">&#39;SAGAClassifier instead.&#39;</span>
                             <span class="s1">&#39;.&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_label_transformers</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">neg_label</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">y_binary</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer_</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_binary</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span></div>


<div class="viewcode-block" id="SAGAClassifier"><a class="viewcode-back" href="../../../generated/lightning.classification.SAGAClassifier.html#lightning.classification.SAGAClassifier">[docs]</a><span class="k">class</span> <span class="nc">SAGAClassifier</span><span class="p">(</span><span class="n">SAGClassifier</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimator for learning linear classifiers by SAGA.</span>

<span class="sd">    Solves the following objective:</span>

<span class="sd">        minimize_w  1 / n_samples * \sum_i loss(w^T x_i, y_i)</span>
<span class="sd">                    + alpha * 0.5 * ||w||^2_2 + beta * penalty(w)</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    eta : float or {&#39;auto&#39;, &#39;line-search&#39;}, defaults to &#39;auto&#39;</span>
<span class="sd">        step size for the gradient updates. If set to &#39;auto&#39;,</span>
<span class="sd">        this will calculate a step size based on the input data.</span>
<span class="sd">        If set to &#39;line-search&#39;, it will perform a line-search</span>
<span class="sd">        to find the step size based for the current iteration.</span>
<span class="sd">    alpha : float</span>
<span class="sd">        amount of squared L2 regularization</span>
<span class="sd">    beta : float</span>
<span class="sd">        amount of regularization for the penalty term</span>
<span class="sd">    loss : string</span>
<span class="sd">        loss to use in the objective function. Can be one of</span>
<span class="sd">        &quot;smooth_hinge&quot;, &quot;squared_hinge&quot; or &quot;log&quot; (for logistic loss).</span>
<span class="sd">    penalty : string or Penalty object</span>
<span class="sd">        penalty term to use in the objective function. Can be &quot;l1&quot;</span>
<span class="sd">        or a custom Penalty object (object defined in</span>
<span class="sd">        lightning/impl/sag_fast.pxd)</span>
<span class="sd">    gamma : float</span>
<span class="sd">        gamma parameter in the &quot;smooth_hinge&quot; loss (not used for other</span>
<span class="sd">        loss functions)</span>
<span class="sd">    max_iter : int</span>
<span class="sd">        maximum number of outer iterations (also known as epochs).</span>
<span class="sd">    tol : float</span>
<span class="sd">        stopping criterion tolerance.</span>
<span class="sd">    verbose : int</span>
<span class="sd">        verbosity level. Set positive to print progress information.</span>
<span class="sd">    callback : callable or None</span>
<span class="sd">        if given, callback(self) will be called on each outer iteration</span>
<span class="sd">        (epoch).</span>
<span class="sd">    random_state: int or RandomState</span>
<span class="sd">        Pseudo-random number generator state used for random sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="SAGAClassifier.__init__"><a class="viewcode-back" href="../../../generated/lightning.classification.SAGAClassifier.html#lightning.classification.SAGAClassifier.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;smooth_hinge&quot;</span><span class="p">,</span>
                 <span class="n">penalty</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>  <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_inner</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">SAGAClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span>
                <span class="n">eta</span><span class="o">=</span><span class="n">eta</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="n">penalty</span><span class="p">,</span>
                <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">n_inner</span><span class="o">=</span><span class="n">n_inner</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">callback</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_saga</span> <span class="o">=</span> <span class="bp">True</span></div></div>


<div class="viewcode-block" id="SAGRegressor"><a class="viewcode-back" href="../../../generated/lightning.regression.SAGRegressor.html#lightning.classification.SAGRegressor">[docs]</a><span class="k">class</span> <span class="nc">SAGRegressor</span><span class="p">(</span><span class="n">BaseRegressor</span><span class="p">,</span> <span class="n">_BaseSAG</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimator for learning linear regressors by SAG.</span>

<span class="sd">    Solves the following objective:</span>

<span class="sd">        minimize_w  1 / n_samples * \sum_i loss(w^T x_i, y_i)</span>
<span class="sd">                    + alpha * 0.5 * ||w||^2_2</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    eta : float or {&#39;auto&#39;, &#39;line-search&#39;}, defaults to &#39;auto&#39;</span>
<span class="sd">        step size for the gradient updates. If set to &#39;auto&#39;,</span>
<span class="sd">        this will calculate a step size based on the input data.</span>
<span class="sd">        If set to &#39;line-search&#39;, it will perform a line-search</span>
<span class="sd">        to find the step size based for the current iteration.</span>
<span class="sd">    alpha : float</span>
<span class="sd">        amount of squared L2 regularization.</span>
<span class="sd">    beta : float</span>
<span class="sd">        amount of regularization for the penalty term.</span>
<span class="sd">    loss : string</span>
<span class="sd">        loss to use in the objective function. Can be &quot;modified_huber&quot; or</span>
<span class="sd">        &quot;squared&quot;.</span>
<span class="sd">    max_iter : int</span>
<span class="sd">        maximum number of outer iterations (also known as epochs).</span>
<span class="sd">    tol : float</span>
<span class="sd">        stopping criterion tolerance.</span>
<span class="sd">    verbose : int</span>
<span class="sd">        verbosity level. Set positive to print progress information.</span>
<span class="sd">    callback : callable or None</span>
<span class="sd">        if given, callback(self) will be called on each outer iteration</span>
<span class="sd">        (epoch).</span>
<span class="sd">    random_state: int or RandomState</span>
<span class="sd">        Pseudo-random number generator state used for random sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="SAGRegressor.__init__"><a class="viewcode-back" href="../../../generated/lightning.regression.SAGRegressor.html#lightning.classification.SAGRegressor.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;smooth_hinge&quot;</span><span class="p">,</span>
                 <span class="n">penalty</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_inner</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">eta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">=</span> <span class="n">penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_inner</span> <span class="o">=</span> <span class="n">n_inner</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback</span> <span class="o">=</span> <span class="n">callback</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_saga</span> <span class="o">=</span> <span class="bp">False</span></div>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_saga</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Penalties are not supported in SAGRegressor. &#39;</span>
                             <span class="s1">&#39;Please use SAGARegressor instead.&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outputs_2d_</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs_2d_</span> <span class="k">else</span> <span class="n">y</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span></div>


<div class="viewcode-block" id="SAGARegressor"><a class="viewcode-back" href="../../../generated/lightning.regression.SAGARegressor.html#lightning.classification.SAGARegressor">[docs]</a><span class="k">class</span> <span class="nc">SAGARegressor</span><span class="p">(</span><span class="n">SAGRegressor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimator for learning linear regressors by SAG.</span>

<span class="sd">    Solves the following objective:</span>

<span class="sd">        minimize_w  1 / n_samples * \sum_i loss(w^T x_i, y_i)</span>
<span class="sd">                    + alpha * 0.5 * ||w||^2_2 + beta * penalty(w)</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    eta : float or {&#39;auto&#39;, &#39;line-search&#39;}, defaults to &#39;auto&#39;</span>
<span class="sd">        step size for the gradient updates. If set to &#39;auto&#39;,</span>
<span class="sd">        this will calculate a step size based on the input data.</span>
<span class="sd">        If set to &#39;line-search&#39;, it will perform a line-search</span>
<span class="sd">        to find the step size based for the current iteration.</span>
<span class="sd">    alpha : float</span>
<span class="sd">        amount of squared L2 regularization</span>
<span class="sd">    beta : float</span>
<span class="sd">        amount of regularization for the penalty term</span>
<span class="sd">    loss : string</span>
<span class="sd">        loss to use in the objective function. Can be &quot;modified_huber&quot; or</span>
<span class="sd">        &quot;squared&quot;.</span>
<span class="sd">    penalty : string or Penalty object</span>
<span class="sd">        penalty term to use in the objective function. Can be &quot;l1&quot;</span>
<span class="sd">        or a custom Penalty object (object defined in</span>
<span class="sd">        lightning/impl/sag_fast.pxd)</span>
<span class="sd">    max_iter : int</span>
<span class="sd">        maximum number of outer iterations (also known as epochs).</span>
<span class="sd">    tol : float</span>
<span class="sd">        stopping criterion tolerance.</span>
<span class="sd">    verbose : int</span>
<span class="sd">        verbosity level. Set positive to print progress information.</span>
<span class="sd">    callback : callable or None</span>
<span class="sd">        if given, callback(self) will be called on each outer iteration</span>
<span class="sd">        (epoch).</span>
<span class="sd">    random_state: int or RandomState</span>
<span class="sd">        Pseudo-random number generator state used for random sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="SAGARegressor.__init__"><a class="viewcode-back" href="../../../generated/lightning.regression.SAGARegressor.html#lightning.classification.SAGARegressor.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;smooth_hinge&quot;</span><span class="p">,</span>
                 <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_inner</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">SAGARegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span>
                <span class="n">eta</span><span class="o">=</span><span class="n">eta</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="n">penalty</span><span class="p">,</span>
                <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">n_inner</span><span class="o">=</span><span class="n">n_inner</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">callback</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_saga</span> <span class="o">=</span> <span class="bp">True</span></div></div>
</pre></div>

</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright Mathieu Blondel.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.5.<br/>
    </p>
  </div>
</footer>
  </body>
</html>